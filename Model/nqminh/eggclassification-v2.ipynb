{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7490549,"sourceType":"datasetVersion","datasetId":4361124},{"sourceId":13576130,"sourceType":"datasetVersion","datasetId":8624610}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Data augumetation","metadata":{}},{"cell_type":"code","source":"# import \nimport os\nimport torch\nfrom torchvision import transforms, datasets\nfrom PIL import Image\n\n# define new and old data path\ndataset_old = '/kaggle/input/eggs-images-classification-damaged-or-not/Eggs Classification'\ndataset_new = '/kaggle/working/egg-augument'\n\n# define the transform layer\naugumentation = transforms.Compose([\n    transforms.RandomRotation(degrees=15),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomResizedCrop(size=224, scale=(0.8, 1.0)),\n])\n\noriginal_transform = transforms.Compose([ # turn original image into 224x224\n    transforms.Resize((224,224))\n])\n\n# add class in augument dir\nfor class_name in os.listdir(dataset_old):\n    os.makedirs(os.path.join(dataset_new, class_name), exist_ok=True)\n\n# repeated transform in the original dataset\ndamaged_iter = 3;\nnot_damaged_iter = 12;\n\nfor class_name in os.listdir(dataset_old):\n    class_path = os.path.join(dataset_old, class_name)\n    output_path = os.path.join(dataset_new, class_name)\n    \n    for file_name in os.listdir(class_path):\n        file_path = os.path.join(class_path, file_name)\n        image = original_transform(Image.open(file_path).convert('RGB'))\n        image.save(os.path.join(output_path, file_name))\n\n        if class_name == 'Damaged':\n            for i in range(damaged_iter):\n                augumented_image = augumentation(image)\n                new_name = f\"{os.path.splitext(file_name)[0]}_aug_{i}.jpg\"\n                augumented_image.save(os.path.join(output_path, new_name))\n\n        if class_name == 'Not Damaged':\n            for i in range(not_damaged_iter):\n                augumented_image = augumentation(image)\n                new_name = f\"{os.path.splitext(file_name)[0]}_aug_{i}.jpg\"\n                augumented_image.save(os.path.join(output_path, new_name))\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T09:48:38.199293Z","iopub.execute_input":"2025-11-01T09:48:38.200085Z","iopub.status.idle":"2025-11-01T09:49:47.280823Z","shell.execute_reply.started":"2025-11-01T09:48:38.200051Z","shell.execute_reply":"2025-11-01T09:49:47.279859Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!zip -r /kaggle/working/egg-augument.zip /kaggle/working/egg-augument ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Split data","metadata":{}},{"cell_type":"code","source":"print(f\"Not damage: {len(os.listdir('/kaggle/working/egg-augument/Not Damaged'))}\")\nprint(f\"Damage: {len(os.listdir('/kaggle/working/egg-augument/Damaged'))}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T09:30:49.351118Z","iopub.execute_input":"2025-11-01T09:30:49.351909Z","iopub.status.idle":"2025-11-01T09:30:49.360793Z","shell.execute_reply.started":"2025-11-01T09:30:49.351872Z","shell.execute_reply":"2025-11-01T09:30:49.359594Z"}},"outputs":[{"name":"stdout","text":"Not damage: 2106\nDamage: 2528\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from torch.utils.data import random_split, DataLoader\n\ntransform_tensor = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])\n\ndataset = datasets.ImageFolder(root='/kaggle/input/new-egg/egg-augument', transform=transform_tensor)\n\n# set train and val size\ntrain_size = int(0.8 * len(dataset))\nval_size = int(len(dataset) - train_size)\n\nprint(f\"train size: {train_size}\")\nprint(f\"val size: {val_size}\")\n\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T10:05:35.016969Z","iopub.execute_input":"2025-11-01T10:05:35.017686Z","iopub.status.idle":"2025-11-01T10:05:36.314535Z","shell.execute_reply.started":"2025-11-01T10:05:35.017655Z","shell.execute_reply":"2025-11-01T10:05:36.313793Z"}},"outputs":[{"name":"stdout","text":"train size: 3707\nval size: 927\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Load VGG16 (unfreeze the last CNN layer and classification layers)","metadata":{}},{"cell_type":"code","source":"from torchvision import models\nimport torch.nn as nn\nimport torch.optim as optim\n\nvgg16 = models.vgg16(pretrained=True)\nprint(vgg16)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-01T11:03:24.446151Z","iopub.execute_input":"2025-11-01T11:03:24.446612Z","iopub.status.idle":"2025-11-01T11:03:26.932765Z","shell.execute_reply.started":"2025-11-01T11:03:24.446581Z","shell.execute_reply":"2025-11-01T11:03:26.932015Z"}},"outputs":[{"name":"stdout","text":"VGG(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (6): ReLU(inplace=True)\n    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): ReLU(inplace=True)\n    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (13): ReLU(inplace=True)\n    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (15): ReLU(inplace=True)\n    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (18): ReLU(inplace=True)\n    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (20): ReLU(inplace=True)\n    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (22): ReLU(inplace=True)\n    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (25): ReLU(inplace=True)\n    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (27): ReLU(inplace=True)\n    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (29): ReLU(inplace=True)\n    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n  (classifier): Sequential(\n    (0): Linear(in_features=25088, out_features=4096, bias=True)\n    (1): ReLU(inplace=True)\n    (2): Dropout(p=0.5, inplace=False)\n    (3): Linear(in_features=4096, out_features=4096, bias=True)\n    (4): ReLU(inplace=True)\n    (5): Dropout(p=0.5, inplace=False)\n    (6): Linear(in_features=4096, out_features=1000, bias=True)\n  )\n)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# set freeze and unfreeze to last cnn layer and classfication layers\nfor param in vgg16.features[:24].parameters():\n    param.requires_grad = False\n\nvgg16.classifier[6] = nn.Linear(4096,2)\n\n# optimizer\noptimizer = optim.Adam(\n    filter(lambda p: p.requires_grad, vgg16.parameters()),\n    lr = 1e-14\n)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nvgg16 = vgg16.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T11:03:48.619412Z","iopub.execute_input":"2025-11-01T11:03:48.620372Z","iopub.status.idle":"2025-11-01T11:03:48.639786Z","shell.execute_reply.started":"2025-11-01T11:03:48.620339Z","shell.execute_reply":"2025-11-01T11:03:48.638789Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}